{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cascade ST with Huggingface Transformers (HF)\n",
    "\n",
    "This notebook provides the training script used in our research paper.\n",
    "\n",
    "It utilizes pre-processed data generated by ESPnet to train a speech-to-text model. The data includes audio file paths and corresponding text transcripts. The audio file paths may point to either the original or copied versions of the audio files.\n",
    "\n",
    "In this notebook, we will use the Huggingface Transformers (HF) adapter to fine-tune the pre-trained OWSM model. Basically everything is the same as the `train.ipynb` except for the custom fine-tuning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from espnet2.layers.create_adapter_fn import create_lora_adapter\n",
    "from espnet2.asr.espnet_model import ESPnetASRModel\n",
    "from espnet2.train.dataset import kaldi_loader\n",
    "from espnet2.train.abs_espnet_model import AbsESPnetModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import espnetez as ez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the paths to your dumped files and other important training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"path/to/egs2/must_c_v2/ez1\")\n",
    "\n",
    "CONFIG = \"owsm_finetune_base\"\n",
    "FINETUNE_MODEL = \"pyf98/librispeech_100_e_branchformer\"\n",
    "HF_MODEl = \"google-t5/t5-base\"\n",
    "\n",
    "DATA_PATH = f\"{BASE_DIR}/data\"\n",
    "DUMP_DIR = f\"{BASE_DIR}/dump/raw\"\n",
    "STATS_DIR = f\"{BASE_DIR}/exp/stats_huggingface_cascade\"\n",
    "EXP_DIR = f\"{BASE_DIR}/exp/train_huggingface_cascade\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a custom dataset class to retrieve data from the dumped files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class to load data from preprocessed files\n",
    "class CustomDataset:\n",
    "    def __init__(self, data_path, is_train=True):\n",
    "        self.data_path = data_path\n",
    "        if is_train:\n",
    "            data_path = f\"{data_path}/train.en-de\"\n",
    "        else:\n",
    "            data_path = f\"{data_path}/dev.en-de\"\n",
    "        \n",
    "        self.data = {}\n",
    "        with open(f\"{data_path}/text.tc.de\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, translated = line.strip().split(maxsplit=1)\n",
    "                translated = translated.replace(\" &apos;\", \"'\")\\\n",
    "                                       .replace(\" &quot;\", '\"')\\\n",
    "                                       .replace(\" &amp;\", \"&\")\n",
    "                self.data[audio_id] = {\n",
    "                    'translated': translated\n",
    "                }\n",
    "        \n",
    "        with open(f\"{data_path}/text\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, text = line.strip().split(maxsplit=1)\n",
    "                text = text.replace(\" &apos;\", \"'\")\\\n",
    "                           .replace(\" &quot;\", '\"')\\\n",
    "                           .replace(\" &amp;\", \"&\")\n",
    "                self.data[audio_id]['text'] = text\n",
    "        \n",
    "        self.keys = list(self.data.keys())[1:]\n",
    "        self.loader = kaldi_loader(f\"{data_path}/wav.scp\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == int:\n",
    "            idx = int(idx)\n",
    "            return {\n",
    "                'speech': self.loader[idx][1].astype(np.float32),\n",
    "                'text': self.data[self.keys[idx]]['text'],\n",
    "                'translated': self.data[self.keys[idx]]['translated']\n",
    "            }\n",
    "        return {\n",
    "            'speech': self.loader[idx][1].astype(np.float32),\n",
    "            'text': self.data[idx]['text'],\n",
    "            'translated': self.data[idx]['translated']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a custom class for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFinetuneModel(AbsESPnetModel):\n",
    "    def __init__(self, nbest=5, beam_size=10, log_every=500):\n",
    "        super().__init__()\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.log_every = log_every\n",
    "        self.asr_model = Speech2Text.from_pretrained(\n",
    "            FINETUNE_MODEL,\n",
    "            nbest=nbest,\n",
    "            beam_size=beam_size,\n",
    "            device=device\n",
    "        )\n",
    "        self.lm = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            HF_MODEl,\n",
    "            device_map = device\n",
    "        )\n",
    "        self.lm_tokenizer = AutoTokenizer.from_pretrained(HF_MODEl)\n",
    "        self.log_stats = {\n",
    "            'loss': 0\n",
    "        }\n",
    "        self.iter_count = 0\n",
    "\n",
    "    def collect_feats(\n",
    "        self,\n",
    "        speech: torch.Tensor,\n",
    "        speech_lengths: torch.Tensor,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return {\"feats\": speech, \"feats_lengths\": speech_lengths}\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        speech: torch.Tensor,\n",
    "        speech_lengths: torch.Tensor,\n",
    "        text: torch.Tensor,\n",
    "        text_lengths: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # 1. ASR\n",
    "        asr_texts = []\n",
    "        for i in range(len(speech)):\n",
    "            asr_texts = self.asr_model(speech[i][:speech_lengths[i]])[0][0]\n",
    "            asr_texts.append(\"translate English to German: \" + asr_texts.capitalize())\n",
    "\n",
    "        # compute hf loss\n",
    "        target_tokens = self.lm_tokenizer(\n",
    "            asr_texts, return_tensors=\"pt\").input_ids.to(speech.device)\n",
    "        lm_output = self.lm(input_ids=target_tokens, labels=text)\n",
    "\n",
    "        # Add lm loss to ASR loss\n",
    "        loss = lm_output.loss\n",
    "        self.log_stats['loss'] += loss.item()\n",
    "        stats = {\n",
    "            'loss': loss.detach()\n",
    "        }\n",
    "\n",
    "        self.iter_count += 1\n",
    "        if self.iter_count % self.log_every == 0:\n",
    "            _loss = self.log_stats['loss'] / self.log_every\n",
    "            print(f\"[{self.iter_count}] - loss: {_loss:.3f}\")\n",
    "            self.log_stats['loss'] = 0.0\n",
    "\n",
    "        return loss, stats, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the training configuration. This involves using the configuration from a pre-trained OWSM model as a base and then adding our own custom configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained OWSM model configuration for tokenizer, converter, and base training configuration\n",
    "from espnet2.bin.s2t_inference import Speech2Text\n",
    "\n",
    "pretrained_model = Speech2Text.from_pretrained(\n",
    "    FINETUNE_MODEL,\n",
    ")\n",
    "tokenizer = pretrained_model.tokenizer\n",
    "converter = pretrained_model.converter\n",
    "training_config = vars(pretrained_model.s2t_train_args)\n",
    "del pretrained_model\n",
    "\n",
    "# Update finetuning configuration from a YAML file (likely user-defined)\n",
    "finetune_config = ez.config.update_finetune_config(\n",
    "    \"s2t\",\n",
    "    training_config,\n",
    "    \"path/to/your/finetune/config.yaml\"\n",
    ")\n",
    "finetune_config['multiple_iterator'] = False  # Set training parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define `data_info` to connect our custom dataset with the ESPnet dataloader. This helps the dataloader understand how to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data_info to connect custom dataset with ESPnet dataloader\n",
    "lm_tokenizer = AutoTokenizer.from_pretrained(HF_MODEl)\n",
    "data_info = {\n",
    "    \"speech\": lambda d : d['speech'],\n",
    "    \"text\": lambda d : lm_tokenizer(d['translated'].upper(), return_tensors=\"np\").input_ids,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to prepare the model for fine-tuning. While you can define a custom model here, this notebook uses the pre-trained OWSM model directly.\n",
    "\n",
    "Note that if we don't need the preprocessing steps during `collect_stats` process, we can set `build_preprocess_fn` to return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fn(args):\n",
    "    model = CustomFinetuneModel(log_every=20)\n",
    "    return model\n",
    "\n",
    "def build_preprocess_fn(*args, **kwargs):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready!\n",
    "Now, let's prepare the datasets and convert them into the format expected by ESPnet's dataloader. The dataloader relies on ESPnetDataset objects to process and feed data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=True)\n",
    "dev_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=False)\n",
    "\n",
    "train_dataset = ez.dataset.ESPnetEZDataset(train_dataset, data_info=data_info)\n",
    "dev_dataset = ez.dataset.ESPnetEZDataset(dev_dataset, data_info=data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the training configuration a little bit to avoid ESPnet to search tokenizer-related files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_config['token_list'] = []\n",
    "finetune_config['token_type'] = 'char'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, let's start the training process!\n",
    "\n",
    "We set the task as `asr` since the model takes the same input as the ASR, and use the same loss function and training process as the ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ez.Trainer(\n",
    "    task=\"asr\",\n",
    "    train_config=finetune_config,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=dev_dataset,\n",
    "    data_info=data_info,\n",
    "    build_model_fn=build_model_fn,\n",
    "    build_preprocess_fn=build_preprocess_fn,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1,\n",
    ")\n",
    "trainer.collect_stats()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
