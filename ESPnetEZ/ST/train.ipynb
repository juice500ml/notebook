{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with MuST-C\n",
    "\n",
    "This notebook provides the training script used in our research paper.\n",
    "\n",
    "It utilizes pre-processed data generated by ESPnet to train a speech-to-text model. The data includes audio file paths and corresponding text transcripts. The audio file paths may point to either the original or copied versions of the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "import espnetez as ez\n",
    "from espnet2.asr.espnet_model import ESPnetASRModel\n",
    "from espnet2.train.dataset import kaldi_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the paths to your dumped files and other important training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to project directories and pre-trained model\n",
    "BASE_DIR = Path(\"path/to/egs2/must_c_v2/ez1\")\n",
    "\n",
    "DATA_PATH = f\"{BASE_DIR}/data\"\n",
    "DUMP_DIR = f\"{BASE_DIR}/dump/raw\"\n",
    "STATS_DIR = f\"{BASE_DIR}/exp/stats_owsm_base_finetune\"\n",
    "EXP_DIR = f\"{BASE_DIR}/exp/owsm_base_finetune\"\n",
    "\n",
    "FINETUNE_MODEL = \"espnet/owsm_v3.1_ebf_base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a custom dataset class to retrieve data from the dumped files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class to load data from preprocessed files\n",
    "class CustomDataset:\n",
    "    def __init__(self, data_path, is_train=True):\n",
    "        self.data_path = data_path\n",
    "        if is_train:\n",
    "            data_path = f\"{data_path}/train.en-de\"\n",
    "        else:\n",
    "            data_path = f\"{data_path}/dev.en-de\"\n",
    "        \n",
    "        self.data = {}\n",
    "        with open(f\"{data_path}/text.tc.de\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, translated = line.strip().split(maxsplit=1)\n",
    "                translated = translated.replace(\" &apos;\", \"'\")\\\n",
    "                                       .replace(\" &quot;\", '\"')\\\n",
    "                                       .replace(\" &amp;\", \"&\")\n",
    "                self.data[audio_id] = {\n",
    "                    'translated': translated\n",
    "                }\n",
    "        \n",
    "        with open(f\"{data_path}/text\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, text = line.strip().split(maxsplit=1)\n",
    "                text = text.replace(\" &apos;\", \"'\")\\\n",
    "                           .replace(\" &quot;\", '\"')\\\n",
    "                           .replace(\" &amp;\", \"&\")\n",
    "                self.data[audio_id]['text'] = text\n",
    "        \n",
    "        self.keys = list(self.data.keys())[1:]\n",
    "        self.loader = kaldi_loader(f\"{data_path}/wav.scp\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == int:\n",
    "            idx = int(idx)\n",
    "            return {\n",
    "                'speech': self.loader[idx][1].astype(np.float32),\n",
    "                'text': self.data[self.keys[idx]]['text'],\n",
    "                'translated': self.data[self.keys[idx]]['translated']\n",
    "            }\n",
    "        return {\n",
    "            'speech': self.loader[idx][1].astype(np.float32),\n",
    "            'text': self.data[idx]['text'],\n",
    "            'translated': self.data[idx]['translated']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the OWSM models were trained on lowercase text, we need to convert all text data to lowercase. \n",
    "\n",
    "Next, we'll define the training configuration. This involves using the configuration from a pre-trained OWSM model as a base and then adding our own custom configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained OWSM model configuration for tokenizer, converter, and base training configuration\n",
    "from espnet2.bin.s2t_inference import Speech2Text\n",
    "\n",
    "pretrained_model = Speech2Text.from_pretrained(\n",
    "    FINETUNE_MODEL,\n",
    "    # category_sym=\"<en>\",  # Comment out if not used\n",
    "    beam_size=10,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "tokenizer = pretrained_model.tokenizer\n",
    "converter = pretrained_model.converter\n",
    "training_config = vars(pretrained_model.s2t_train_args)\n",
    "del pretrained_model\n",
    "\n",
    "# Update finetuning configuration from a YAML file (likely user-defined)\n",
    "finetune_config = ez.config.update_finetune_config(\n",
    "    \"s2t\",\n",
    "    training_config,\n",
    "    \"path/to/your/finetune/config.yaml\"\n",
    ")\n",
    "finetune_config['multiple_iterator'] = False  # Set training parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define `data_info` to connect our custom dataset with the ESPnet dataloader. This helps the dataloader understand how to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data_info to connect custom dataset with ESPnet dataloader\n",
    "def tokenize(text):\n",
    "    return np.array(converter.tokens2ids(tokenizer.text2tokens(text)))\n",
    "\n",
    "\n",
    "data_info = {\n",
    "    \"speech\": lambda d : d['speech'],\n",
    "    \"text\": lambda d : tokenize(f\"<eng><st_deu><notimestamps> {d['translated']}\"),\n",
    "    \"text_prev\": lambda d : tokenize(\"<na>\"),\n",
    "    \"text_ctc\": lambda d : tokenize(d['text'].lower()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to prepare the model for fine-tuning. While you can define a custom model here, this notebook uses the pre-trained OWSM model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def build_model_fn(args):\n",
    "    pretrained_model = Speech2Text.from_pretrained(\n",
    "        FINETUNE_MODEL,\n",
    "        beam_size=10,\n",
    "    )\n",
    "    model = pretrained_model.s2t_model\n",
    "    model.train()\n",
    "    print(f'Trainable parameters: {count_parameters(model)}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready!\n",
    "Now, let's prepare the datasets and convert them into the format expected by ESPnet's dataloader. The dataloader relies on ESPnetDataset objects to process and feed data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=True)\n",
    "dev_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=False)\n",
    "\n",
    "train_dataset = ez.dataset.ESPnetEZDataset(train_dataset, data_info=data_info)\n",
    "dev_dataset = ez.dataset.ESPnetEZDataset(dev_dataset, data_info=data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, let's start the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ez.Trainer(\n",
    "    task=\"s2t\",\n",
    "    train_config=finetune_config,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=dev_dataset,\n",
    "    data_info=data_info,\n",
    "    build_model_fn=build_model_fn,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1,\n",
    ")\n",
    "trainer.collect_stats()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
